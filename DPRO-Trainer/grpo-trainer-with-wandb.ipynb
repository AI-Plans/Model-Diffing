{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q trl openai wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T16:14:44.368790Z","iopub.execute_input":"2025-12-20T16:14:44.369020Z","iopub.status.idle":"2025-12-20T16:14:49.704640Z","shell.execute_reply.started":"2025-12-20T16:14:44.369000Z","shell.execute_reply":"2025-12-20T16:14:49.703703Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# %%writefile train_grpo.py\n\nimport wandb\n\nwandb.login(key=\"TOKEN\")\nos.environ[\"WANDB_RESUME\"] = \"allow\"\nfrom peft import LoraConfig\n\nfrom huggingface_hub import login\n\nlogin(token=\"TOKEN\")\nfrom datasets import load_dataset\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    AutoModelForCausalLM,\n)\nfrom trl import GRPOTrainer, GRPOConfig\nfrom peft import LoraConfig\n\n\n# ======================================================\n# 1. Dataset\n# ======================================================\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nimport numpy as np\n\nds = load_dataset(\"AIPlans/Helpsteer2-helpfulness-prompts\", split=\"train\")\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n\nlengths = [len(tokenizer(x[\"prompt\"]).input_ids) for x in ds]\ncutoff = int(np.percentile(lengths, 90))\n\nds = ds.filter(\n    lambda x: len(tokenizer(x[\"prompt\"]).input_ids) <= cutoff\n)\n\n\nprint(\"90% kept:\", len(ds))\nprint(\"Cutoff tokens:\", cutoff)\n\n# ======================================================\n# 2. Reward model + reward function\n# ======================================================\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"https://integrate.api.nvidia.com/v1\",\n    api_key=\"TOKEN\",\n)\n\ndef reward_model_score(prompts, completions,**kwargs):\n    # print(prompts)\n    # print(completions)\n    # for i, (p, c) in enumerate(zip(prompts, completions)):\n    #         print(f\"[{i}] PROMPT:\\n{p}\")\n    #         print(f\"\\nCOMPLETION:\\n{c}\")\n    #         print(\"\\n\" + \"-\" * 80 + \"\\n\")\n\n    if not prompts:\n        return []\n\n    scores = []\n\n    for prompt, completion in zip(prompts, completions):\n        resp = client.chat.completions.create(\n            model=\"nvidia/llama-3.1-nemotron-70b-reward\",\n            messages=[\n                {\"role\": \"user\", \"content\": prompt},\n                {\"role\": \"assistant\", \"content\": completion},\n            ],\n        )\n\n        content = resp.choices[0].message.content\n\n        # Normalize content to string\n        if not isinstance(content, str):\n            content = \"\".join(part.get(\"text\", \"\") for part in content)\n\n        # Parse reward (handles \"reward: -12.625\" or just \"-12.625\")\n        try:\n            score = float(content.split(\":\", 1)[-1].strip())\n        except ValueError as e:\n            raise ValueError(f\"Could not parse reward from content={content!r}\") from e\n\n        scores.append(score)\n    # print(f'score={scores}')\n    return scores\n\nfrom datasets import load_dataset\nfrom trl import GRPOTrainer, GRPOConfig\nfrom transformers import AutoTokenizer\n\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    target_modules=[\"q_proj\", \"v_proj\"],  # or [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n    task_type=\"CAUSAL_LM\",\n)\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom trl import GRPOConfig, GRPOTrainer\n\nmodel_id = \"Qwen/Qwen3-0.6B\"\n\n# Load the tokenizer\ntok = AutoTokenizer.from_pretrained(model_id)\ntok.padding_side = \"left\"\nif tok.pad_token is None:\n    tok.pad_token = tok.eos_token\n\n\n# Load the model explicitly as requested\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n)\n\ntrain_args = GRPOConfig(\n    output_dir=\"QwenModel\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=6,\n    num_train_epochs=3,\n    max_completion_length=248,  # increase later\n    num_generations=4,          # keep small at first\n    chat_template_kwargs={\"enable_thinking\": False},\n    report_to=\"wandb\",          # This is the key line\n    # log_completions=True,\n    logging_steps=1,\n    save_steps=50,\n    save_total_limit=2,\n)\n\ntrainer = GRPOTrainer(\n    model=model,                # Passing the loaded model object\n    args=train_args,\n    processing_class=tok,\n    reward_funcs=reward_model_score,\n    train_dataset=ds,\n    peft_config=peft_config,\n)\n\ntrainer.train(resume_from_checkpoint=\"QwenModel/checkpoint-3249\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-20T16:25:42.464615Z","iopub.execute_input":"2025-12-20T16:25:42.465446Z","iopub.status.idle":"2025-12-20T16:25:59.312495Z","shell.execute_reply.started":"2025-12-20T16:25:42.465413Z","shell.execute_reply":"2025-12-20T16:25:59.311966Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"90% kept: 6499\nCutoff tokens: 501\n","output_type":"stream"},{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251220_162552-ofk8q7gk</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/jithesh-independent/huggingface/runs/ofk8q7gk' target=\"_blank\">winter-sun-12</a></strong> to <a href='https://wandb.ai/jithesh-independent/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/jithesh-independent/huggingface' target=\"_blank\">https://wandb.ai/jithesh-independent/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/jithesh-independent/huggingface/runs/ofk8q7gk' target=\"_blank\">https://wandb.ai/jithesh-independent/huggingface/runs/ofk8q7gk</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Detected [openai] in use.\n\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3249' max='3249' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3249/3249 : < :, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3249, training_loss=0.0, metrics={'train_runtime': 6.506, 'train_samples_per_second': 2996.789, 'train_steps_per_second': 499.388, 'total_flos': 0.0, 'train_loss': 0.0})"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"trained_model = trainer.model\nfrom peft import PeftModel\n\n# Merge LoRA → base model\nmerged_model = trained_model.merge_and_unload()\nrepo_id = \"AIPlans/Qwen3-0.6B-GRPO-RM_NVIDIA\"\n\nmerged_model.push_to_hub(\n    repo_id,\n    # safe_serialization=True,   # recommended\n)\n\ntok.push_to_hub(repo_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T16:26:23.258345Z","iopub.execute_input":"2025-12-20T16:26:23.258945Z","iopub.status.idle":"2025-12-20T16:26:55.387094Z","shell.execute_reply.started":"2025-12-20T16:26:23.258916Z","shell.execute_reply":"2025-12-20T16:26:55.386487Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8f376a7fe4d42a699264d511a3b6109"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aac9d789b8034dd4a9328e916fda62d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2bad1ed78874ad0b799f5c6f8cf4a32"}},"metadata":{}},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\nWARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"144516016de44867befad510b955d601"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"598b9f01af704bac975d9e156c8f9ba3"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/AIPlans/Qwen3-0.6B-GRPO-RM_NVIDIA/commit/82fc6a441d008ac84a60d8e55df407ff2df170c8', commit_message='Upload tokenizer', commit_description='', oid='82fc6a441d008ac84a60d8e55df407ff2df170c8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/AIPlans/Qwen3-0.6B-GRPO-RM_NVIDIA', endpoint='https://huggingface.co', repo_type='model', repo_id='AIPlans/Qwen3-0.6B-GRPO-RM_NVIDIA'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import os\nos.listdir(\"/kaggle/working/QwenModel\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T16:14:44.360586Z","iopub.execute_input":"2025-12-20T16:14:44.360950Z","iopub.status.idle":"2025-12-20T16:14:44.367409Z","shell.execute_reply.started":"2025-12-20T16:14:44.360925Z","shell.execute_reply":"2025-12-20T16:14:44.366727Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"['checkpoint-3249', 'checkpoint-3200', 'README.md']"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive(\n    base_name=\"/kaggle/working/working_folder\",\n    format=\"zip\",\n    root_dir=\"/kaggle/working\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T18:00:29.485981Z","iopub.execute_input":"2025-12-19T18:00:29.486163Z","iopub.status.idle":"2025-12-19T18:00:34.560101Z","shell.execute_reply.started":"2025-12-19T18:00:29.486143Z","shell.execute_reply":"2025-12-19T18:00:34.559437Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/working_folder.zip'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}