{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "############################################################################################\n",
        "### Before running, add the following keys to your Secrets (left panel):\n",
        "# 1. HF_TOKEN (hugging face)\n",
        "# 2. OPENAI_API_KEY\n",
        "# 3. PINECONE_API_KEY\n",
        "\n",
        "# Note: Notebook will ask to restart after the pip install, this is expecetd behavior\n",
        "# Note: Notebook requires A100 GPU to run (select it in the notbeook settings)\n",
        "# Note: My pinecone embeddings can be used without running the debate yourself (see bottom)\n",
        "############################################################################################"
      ],
      "metadata": {
        "id": "PmZJX2vAQT4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "62X-M4FC5W_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is technically the correct way to install requirements, but I found the explicit way in the cell below less buggy\n",
        "# !pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "xQha96ib3MKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_huggingface langchain_openai langchain_google_genai langchain_community langchain_pinecone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wLUW4AcK2-MG",
        "outputId": "a9c635fc-f24e-4488-b29b-8cfe2eb792c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain_pinecone\n",
            "  Downloading langchain_pinecone-0.2.11-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.3.72)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.21.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.33.4 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.34.1)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.97.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.11.7)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.8)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Collecting pinecone<8.0.0,>=6.0.0 (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone)\n",
            "  Downloading pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting langchain-tests<1.0.0,>=0.3.7 (from langchain_pinecone)\n",
            "  Downloading langchain_tests-0.3.20-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.29.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (25.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (1.1.5)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: pytest<9,>=7 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (8.4.1)\n",
            "Collecting pytest-asyncio<1,>=0.20 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_asyncio-0.26.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (0.28.1)\n",
            "Collecting syrupy<5,>=4 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading syrupy-4.9.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting pytest-socket<1,>=0.6.0 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_socket-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting pytest-benchmark (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_benchmark-5.1.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pytest-codspeed (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_codspeed-4.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (7.3 kB)\n",
            "Collecting pytest-recording (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_recording-0.13.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting vcrpy>=7.0 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading vcrpy-7.0.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (2025.7.14)\n",
            "Collecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone)\n",
            "  Downloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (2.5.0)\n",
            "Collecting aiohttp-retry<3.0.0,>=2.9.1 (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone)\n",
            "  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (3.0.0)\n",
            "Collecting packaging>=20.9 (from huggingface-hub>=0.33.4->langchain_huggingface)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (2.19.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (1.17.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from vcrpy>=7.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.17.2)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from pytest-benchmark->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (9.0.0)\n",
            "Requirement already satisfied: cffi>=1.17.1 in /usr/local/lib/python3.11/dist-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.17.1)\n",
            "Requirement already satisfied: rich>=13.8.1 in /usr/local/lib/python3.11/dist-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (13.9.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.17.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (0.1.2)\n",
            "Downloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
            "Downloading langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.1.8-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_pinecone-0.2.11-py3-none-any.whl (23 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading langchain_tests-0.3.20-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone-7.3.0-py3-none-any.whl (587 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading pytest_asyncio-0.26.0-py3-none-any.whl (19 kB)\n",
            "Downloading pytest_socket-0.7.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading syrupy-4.9.1-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading vcrpy-7.0.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_benchmark-5.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_codspeed-4.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.1/221.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_recording-0.13.4-py3-none-any.whl (13 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: filetype, python-dotenv, pinecone-plugin-interface, packaging, mypy-extensions, httpx-sse, vcrpy, typing-inspect, pinecone-plugin-assistant, marshmallow, syrupy, pytest-socket, pytest-recording, pytest-codspeed, pytest-benchmark, pytest-asyncio, pydantic-settings, pinecone, dataclasses-json, aiohttp-retry, langchain-tests, langchain_openai, langchain_huggingface, google-ai-generativelanguage, langchain_pinecone, langchain_google_genai, langchain_community\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-retry-2.9.1 dataclasses-json-0.6.7 filetype-1.2.0 google-ai-generativelanguage-0.6.18 httpx-sse-0.4.1 langchain-tests-0.3.20 langchain_community-0.3.27 langchain_google_genai-2.1.8 langchain_huggingface-0.3.1 langchain_openai-0.3.28 langchain_pinecone-0.2.11 marshmallow-3.26.1 mypy-extensions-1.1.0 packaging-24.2 pinecone-7.3.0 pinecone-plugin-assistant-1.7.0 pinecone-plugin-interface-0.0.7 pydantic-settings-2.10.1 pytest-asyncio-0.26.0 pytest-benchmark-5.1.0 pytest-codspeed-4.0.0 pytest-recording-0.13.4 pytest-socket-0.7.0 python-dotenv-1.1.1 syrupy-4.9.1 typing-inspect-0.9.0 vcrpy-7.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "packaging"
                ]
              },
              "id": "dccbf0d9fc3b45ccb1e869c688d4d863"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_current_gpu():\n",
        "    is_A100 = !nvidia-smi | grep A100\n",
        "    if is_A100:\n",
        "        print('A100 GPU')\n",
        "    else:\n",
        "      print('This script requires an A100 GPU')\n",
        "      quit()\n",
        "\n",
        "check_current_gpu()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hafFbFL2_IRX",
        "outputId": "d0c77e82-9a8a-4f4b-f0af-8e999bc8daee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A100 GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mistral_common\n",
        "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
        "from mistral_common.protocol.instruct.messages import UserMessage\n",
        "from mistral_common.protocol.instruct.request import ChatCompletionRequest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KILh6QUi_jSP",
        "outputId": "c6417837-1267-49b4-989a-6518e5d8e142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mistral_common\n",
            "  Downloading mistral_common-1.8.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from mistral_common) (2.11.7)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.11/dist-packages (from mistral_common) (4.25.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common) (4.14.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common) (0.9.0)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common) (11.3.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from mistral_common) (2.0.2)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common)\n",
            "  Downloading pydantic_extra_types-2.10.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common) (0.26.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral_common) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral_common) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral_common) (0.4.1)\n",
            "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral_common) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral_common) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral_common) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral_common) (2025.7.14)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.7.0->mistral_common) (2024.11.6)\n",
            "Downloading mistral_common-1.8.3-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.5-py3-none-any.whl (38 kB)\n",
            "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycountry, pydantic-extra-types, mistral_common\n",
            "Successfully installed mistral_common-1.8.3 pycountry-24.6.1 pydantic-extra-types-2.10.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IfmyjFnaRyQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "from huggingface_hub import create_repo\n",
        "from huggingface_hub import HfFolder\n",
        "from google.colab import userdata\n",
        "\n",
        "api = HfApi(token=userdata.get('HF_TOKEN'))"
      ],
      "metadata": {
        "id": "rwEyRcM-8lJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the RedDebate repo\n",
        "!pip install -q transformers accelerate datasets peft bitsandbytes git+https://github.com/huggingface/trl.git\n",
        "!git clone https://github.com/aliasad059/RedDebate.git\n",
        "%cd RedDebate\n",
        "\n",
        "# Get the dataset\n",
        "!wget -q \\\n",
        "  https://raw.githubusercontent.com/centerforaisafety/HarmBench/main/data/behavior_datasets/harmbench_behaviors_text_all.csv \\\n",
        "  -O harmbench.csv"
      ],
      "metadata": {
        "id": "YxZ0tFI02sNX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b7a2d32-1d50-4d4e-cd05-0905f1c7c53e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for trl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Cloning into 'RedDebate'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 29 (delta 4), reused 18 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (29/29), 236.23 KiB | 2.99 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n",
            "/content/RedDebate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "# This is for creating a small version of the dataset, ensuring the new dataset is\n",
        "# representative of the categories in the original (e.g. 'biohazards')\n",
        "def stratified_sample_csv(csv_path, fraction, stratify_col='SemanticCategory'):\n",
        "    \"\"\"\n",
        "    Randomly samples a fraction of rows from a csv in a stratified way\n",
        "    given a column and returns a new csv with the same title as the original\n",
        "    but with the new number of rows added as a suffix.\n",
        "\n",
        "    Args:\n",
        "        csv_path (str): The path to the input CSV file.\n",
        "        stratify_col (str): The name of the column to use for stratification.\n",
        "        fraction (float): The fraction of rows to sample (between 0 and 1).\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the newly created CSV file.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    if stratify_col not in df.columns:\n",
        "        raise ValueError(f\"Column '{stratify_col}' not found in the CSV.\")\n",
        "\n",
        "    if not 0 <= fraction <= 1:\n",
        "        raise ValueError(\"Fraction must be between 0 and 1.\")\n",
        "\n",
        "    # Perform stratified sampling\n",
        "    sampled_df = df.groupby(stratify_col).sample(frac=fraction, random_state=42)\n",
        "\n",
        "    # Create the new file path\n",
        "    base_name, ext = os.path.splitext(csv_path)\n",
        "    # new_file_path = f\"{base_name}_{len(sampled_df)}{ext}\"\n",
        "    new_file_path = f\"{base_name}_small{ext}\"\n",
        "\n",
        "    # Save the sampled dataframe to a new CSV\n",
        "    sampled_df.to_csv(new_file_path, index=False)\n",
        "\n",
        "    return new_file_path\n",
        "\n",
        "harmbench_small = stratified_sample_csv('harmbench.csv', 0.25)\n",
        "\n",
        "############################################################################################\n",
        "# NOTE: Once you have created a dataset, you will want to keep it stored for reproducibility\n",
        "!cp harmbench_small.csv /content/drive/MyDrive/harmbench_small.csv\n",
        "\n",
        "# Run this to retrieve the previously stored dataset\n",
        "# !cp /content/drive/MyDrive/harmbench_small.csv harmbench_small.csv"
      ],
      "metadata": {
        "id": "cCCVPR6UnCVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################################################################\n",
        "# NOTE: The following two cells over-write source code to allow for model quantization\n",
        "############################################################################################"
      ],
      "metadata": {
        "id": "6AoMB7sGR3EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./redDebate/llm.py\n",
        "from typing import Union\n",
        "\n",
        "import torch\n",
        "# device = torch.device('cuda')\n",
        "\n",
        "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
        "from langchain_openai import ChatOpenAI, OpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from langchain_community.llms import VLLM\n",
        "\n",
        "# loads a model from OpenAI\n",
        "class AnyOpenAILLM:\n",
        "    def __init__(self, model_name: str, use_chat: str, prompt_template: PromptTemplate, *args, **kwargs):\n",
        "        self.model_name = model_name\n",
        "        self.prompt_template = prompt_template\n",
        "        self.use_chat = use_chat.lower() == 'true'\n",
        "\n",
        "        if self.use_chat:\n",
        "            self.llm = ChatOpenAI(model=self.model_name, cache=False, **kwargs)\n",
        "        else:\n",
        "            self.llm = OpenAI(model=self.model_name, **kwargs)\n",
        "\n",
        "        # Create a chain\n",
        "        self.chain = self.prompt_template | self.llm\n",
        "\n",
        "    def __call__(self, prompt_inputs: Union[str, dict]):\n",
        "        if self.use_chat:\n",
        "            return self.chain.invoke(prompt_inputs).content\n",
        "        else:\n",
        "            return self.chain.invoke(prompt_inputs)\n",
        "\n",
        "# loads a model from GoogleGenerativeAI\n",
        "class AnyGoogleGenerativeAI:\n",
        "    def __init__(self, model_name: str, use_chat: str, prompt_template: PromptTemplate, *args, **kwargs):\n",
        "        self.model_name = model_name\n",
        "        self.prompt_template = prompt_template\n",
        "        self.use_chat = use_chat.lower() == 'true'\n",
        "\n",
        "        if self.use_chat:\n",
        "            self.llm = ChatGoogleGenerativeAI(model=self.model_name, cache=False, **kwargs)\n",
        "        else:\n",
        "            self.llm = GoogleGenerativeAI(model=self.model_name, **kwargs)\n",
        "\n",
        "        # Create a chain\n",
        "        self.chain = self.prompt_template | self.llm\n",
        "\n",
        "    def __call__(self, prompt_inputs: Union[str, dict]):\n",
        "        if self.use_chat:\n",
        "            return self.chain.invoke(prompt_inputs).content\n",
        "        else:\n",
        "            return self.chain.invoke(prompt_inputs)\n",
        "\n",
        "# loads a model from HuggingFace\n",
        "class AnyHuggingFace:\n",
        "    def __init__(self, model_name: str, use_chat: str, prompt_template: PromptTemplate, *args, **kwargs):\n",
        "        quantization_kwargs = {\n",
        "            'load_in_4bit': kwargs.pop('load_in_4bit', False),\n",
        "            'load_in_8bit': kwargs.pop('load_in_8bit', False),\n",
        "            'bnb_4bit_quant_type': kwargs.pop('bnb_4bit_quant_type', 'nf4'),\n",
        "            'bnb_4bit_compute_dtype': kwargs.pop('bnb_4bit_compute_dtype', None)\n",
        "        }\n",
        "        kwargs.pop('device', None)\n",
        "        final_quant_kwargs = {k: v for k, v in quantization_kwargs.items() if v}\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        print(f\"Loading model '{self.model_name}' with quantization: {final_quant_kwargs}\")\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            trust_remote_code=True,\n",
        "            **final_quant_kwargs\n",
        "        )\n",
        "\n",
        "        self.use_chat = use_chat.lower() == 'true'\n",
        "        self.task = kwargs.get('task', 'text-generation')\n",
        "        # Note: We do not pass `device` here anymore as `device_map` handles placement.\n",
        "        # --- MODIFICATION START ---\n",
        "        # Initialize the pipeline without a device, relying on device_map for the model\n",
        "        self.pipe = pipeline(self.task, model=self.model, tokenizer=self.tokenizer, **kwargs)\n",
        "        # --- MODIFICATION END ---\n",
        "\n",
        "\n",
        "        if self.use_chat:\n",
        "            # --- MODIFICATION START ---\n",
        "            # Initialize ChatHuggingFace with the pipeline\n",
        "            self.llm = ChatHuggingFace(llm=HuggingFacePipeline(pipeline=self.pipe), cache=False)\n",
        "            # --- MODIFICATION END ---\n",
        "        else:\n",
        "            # --- MODIFICATION START ---\n",
        "            # Initialize HuggingFacePipeline\n",
        "            self.llm = HuggingFacePipeline(pipeline=self.pipe)\n",
        "            # --- MODIFICATION END ---\n",
        "\n",
        "        self.prompt_template = prompt_template\n",
        "        self.chain = self.prompt_template | self.llm\n",
        "\n",
        "    def __call__(self, prompt_inputs: Union[str, dict]):\n",
        "        # --- MODIFICATION START ---\n",
        "        # Process prompt and tokenize inputs\n",
        "        inputs = self.prompt_template.invoke(prompt_inputs)\n",
        "\n",
        "        # The Langchain pipeline objects should handle tokenization and device placement\n",
        "        # when the underlying Hugging Face model is on the correct device via device_map.\n",
        "        # Explicitly moving tensors here was causing issues.\n",
        "        # Reverting to the simpler call, trusting Langchain/Pipeline integration\n",
        "        # with the device_map setting on the model itself.\n",
        "        if self.use_chat:\n",
        "             return self.chain.invoke(prompt_inputs).content\n",
        "        else:\n",
        "             return self.chain.invoke(prompt_inputs)\n",
        "        # --- MODIFICATION END ---\n",
        "\n",
        "\n",
        "# loads llama-guard from HuggingFace\n",
        "class LlamaGuard:\n",
        "    def __init__(self, model_name: str, torch_dtype=torch.bfloat16, **kwargs):\n",
        "        self.model_name = model_name\n",
        "        quantization_kwargs = {\n",
        "            'load_in_4bit': kwargs.pop('load_in_4bit', False),\n",
        "            'load_in_8bit': kwargs.pop('load_in_8bit', False),\n",
        "            'bnb_4bit_quant_type': kwargs.pop('bnb_4bit_quant_type', 'nf4'),\n",
        "            'bnb_4bit_compute_dtype': kwargs.pop('bnb_4bit_compute_dtype', None),\n",
        "            \"device_map\": \"auto\"\n",
        "        }\n",
        "        final_quant_kwargs = {k: v for k, v in quantization_kwargs.items() if v}\n",
        "\n",
        "        print(f\"Loading LlamaGuard model '{self.model_name}' with quantization: {final_quant_kwargs}\")\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch_dtype,\n",
        "            **final_quant_kwargs\n",
        "        )\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self.generate_kwargs = kwargs\n",
        "\n",
        "    def __call__(self, text: str):\n",
        "        chat = [{\"role\": \"user\", \"content\": text}]\n",
        "        # --- MODIFICATION START ---\n",
        "        # Ensure input tensors are on the same device as the model before generating\n",
        "        input_ids = self.tokenizer.apply_chat_template(chat, return_tensors=\"pt\")\n",
        "        input_ids = input_ids.to(self.model.device) #input_ids.to('cuda')\n",
        "        # --- MODIFICATION END ---\n",
        "        output = self.model.generate(input_ids=input_ids, **self.generate_kwargs)\n",
        "        prompt_len = input_ids.shape[-1]\n",
        "        return self.tokenizer.decode(output[0][prompt_len:], skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0EGyJSkdwAN",
        "outputId": "b5a9b1d3-9a8d-44aa-94e8-0506222f3ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ./redDebate/llm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ./redDebate/run.py\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from datetime import datetime\n",
        "import json\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "from .debate import Debate, DevilAngelDebate, SocraticDebate\n",
        "from .self_critique import SelfCritique\n",
        "from .agents import DebateAgent, DevilAngelAgent, EvalAgent, FeedbackAgent, HumanAgent, SelfCriticAgent\n",
        "from .memory import ShortTermMemory, LongTermMemory, VectorStoreMemory\n",
        "from .llm import AnyHuggingFace, AnyOpenAILLM, LlamaGuard, AnyGoogleGenerativeAI\n",
        "from .debate_prompts import debate_agent_prompt, devil_debater_prompt, angel_debater_prompt , feedback_prmpt, socratic_agent_prompt, eval_prmpt, init_response_prompt, self_critique_prompt, revise_response_prompt\n",
        "from .dataloader import load_datasets\n",
        "from .utils import setup_logger\n",
        "from .metrics import calculate_debate_metrics, log_results_to_wandb\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# --- Helper functions (save/load checkpoint etc.) are unchanged ---\n",
        "def save_debate_log(debate, checkpoint_folder: str, dataset_name: str, question_idx: int):\n",
        "    os.makedirs(checkpoint_folder, exist_ok=True)\n",
        "    debate_log_path = os.path.join(checkpoint_folder, f\"{dataset_name}_q{question_idx}.json\")\n",
        "    debate.save_to_json(debate_log_path)\n",
        "def save_checkpoint(checkpoint_folder: str, completed_debates: dict):\n",
        "    os.makedirs(checkpoint_folder, exist_ok=True)\n",
        "    checkpoint_path = os.path.join(checkpoint_folder, \"checkpoint.json\")\n",
        "    with open(checkpoint_path, \"w\") as f:\n",
        "        json.dump(completed_debates, f, indent=4)\n",
        "def load_checkpoint(checkpoint_folder: str):\n",
        "    checkpoint_path = os.path.join(checkpoint_folder, \"checkpoint.json\")\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        with open(checkpoint_path, \"r\") as f:\n",
        "            return json.load(f)\n",
        "    return {}\n",
        "def load_saved_debates(checkpoint_folder: str, conversation_type: str ='debate'):\n",
        "    debates = []\n",
        "    if not os.path.exists(checkpoint_folder): return debates\n",
        "    if conversation_type == 'debate':\n",
        "        for file in Path(checkpoint_folder).glob(\"*.json\"):\n",
        "            if file.name != \"checkpoint.json\": debates.append(Debate.load_from_json(str(file)))\n",
        "    elif conversation_type == 'selfcritique':\n",
        "        for file in Path(checkpoint_folder).glob(\"*.json\"):\n",
        "            if file.name != \"checkpoint.json\": debates.append(SelfCritique.load_from_json(str(file)))\n",
        "    return debates\n",
        "# --- End of helper functions ---\n",
        "\n",
        "\n",
        "def init_agents(debater_models, devil_model, angel_model, evaluator_model, feedback_generator, questioner_model, self_critique_model, logger):\n",
        "    loaded_llms = {}\n",
        "\n",
        "    # Common quantization arguments for all Hugging Face models\n",
        "    hf_quant_args = {\n",
        "        \"load_in_4bit\": True,\n",
        "        \"bnb_4bit_quant_type\": \"nf4\",\n",
        "        \"bnb_4bit_compute_dtype\": torch.bfloat16\n",
        "    }\n",
        "\n",
        "    debate_agents = []\n",
        "    for i, model in enumerate(debater_models):\n",
        "        model_type, model_name, use_chat = model.split(':')\n",
        "        if model_type == 'huggingface':\n",
        "            logger.info(f\"Loading HuggingFace model: '{model_name}' as Agent-{i}\")\n",
        "            llm = AnyHuggingFace(\n",
        "                model_name=model_name, use_chat=use_chat, prompt_template=debate_agent_prompt,\n",
        "                do_sample=True, temperature=0.7, top_p=0.9, max_new_tokens=512,\n",
        "                return_full_text=False, ##\n",
        "                **hf_quant_args\n",
        "            )\n",
        "        else: # OpenAI, Google, etc.\n",
        "             llm = AnyOpenAILLM(model_name=model_name, use_chat=use_chat, prompt_template=debate_agent_prompt) # Simplified for brevity\n",
        "\n",
        "        debate_agents.append(DebateAgent(name=f'Agent-{i}', base_llm=llm))\n",
        "\n",
        "    # Evaluator agent\n",
        "    eval_agent = None\n",
        "    if evaluator_model:\n",
        "        evaluator_model_type, evaluator_model_name, evaluator_use_chat = evaluator_model.split(':')\n",
        "        if 'meta-llama/Llama-Guard' in evaluator_model_name:\n",
        "            logger.info(f\"Loading LlamaGuard model: '{evaluator_model_name}' as Agent-Eval\")\n",
        "            # <-- MODIFICATION: Pass quantization args to the LlamaGuard class\n",
        "            eval_llm = LlamaGuard(\n",
        "                model_name=evaluator_model_name,\n",
        "                max_new_tokens=20,\n",
        "                pad_token_id=0,\n",
        "                # return_full_text=False, ##\n",
        "                **hf_quant_args\n",
        "            )\n",
        "            eval_agent = EvalAgent(name='Agent-Eval', base_llm=eval_llm)\n",
        "        elif evaluator_model_type == 'huggingface':\n",
        "            logger.info(f\"Loading HuggingFace model: '{evaluator_model_name}' as Agent-Eval\")\n",
        "            eval_llm = AnyHuggingFace(\n",
        "                model_name=evaluator_model_name, use_chat=evaluator_use_chat, prompt_template=eval_prmpt,\n",
        "                do_sample=True, temperature=0.3, top_p=0.8,\n",
        "                # return_full_text=False, ##\n",
        "                **hf_quant_args\n",
        "            )\n",
        "            eval_agent = EvalAgent(name='Agent-Eval', base_llm=eval_llm)\n",
        "        else: # OpenAI, etc.\n",
        "            if evaluator_model_name == 'moderation':\n",
        "                 eval_agent = EvalAgent(name='Agent-Eval', base_llm=None)\n",
        "            else:\n",
        "                 eval_llm = AnyOpenAILLM(model_name=evaluator_model_name, use_chat=evaluator_use_chat, prompt_template=eval_prmpt)\n",
        "                 eval_agent = EvalAgent(name='Agent-Eval', base_llm=eval_llm)\n",
        "\n",
        "\n",
        "    # Feedback agent\n",
        "    feedback_agent = None\n",
        "    if feedback_generator:\n",
        "        feedback_generator_type, feedback_generator_name, feedback_generator_use_chat = feedback_generator.split(':')\n",
        "        if feedback_generator_type == 'huggingface':\n",
        "            logger.info(f\"Loading HuggingFace model: '{feedback_generator_name}' as Agent-Feedback\")\n",
        "            feedback_llm = AnyHuggingFace(\n",
        "                model_name=feedback_generator_name, use_chat=feedback_generator_use_chat, prompt_template=feedback_prmpt,\n",
        "                do_sample=True, temperature=0.3, top_p=0.8,\n",
        "                return_full_text=False, ##\n",
        "                **hf_quant_args\n",
        "            )\n",
        "            feedback_agent = FeedbackAgent(name='Agent-Feedback', base_llm=feedback_llm)\n",
        "        else: # OpenAI, etc.\n",
        "            feedback_llm = AnyOpenAILLM(model_name=feedback_generator_name, use_chat=feedback_generator_use_chat, prompt_template=feedback_prmpt)\n",
        "            feedback_agent = FeedbackAgent(name='Agent-Feedback', base_llm=feedback_llm)\n",
        "\n",
        "    # Return all agents - other agents (devil, angel, etc.) are omitted for brevity but would follow the same pattern\n",
        "    return debate_agents, None, None, eval_agent, feedback_agent, [], None, None\n",
        "\n",
        "\n",
        "def run_debate(debater_models, devil_model, angel_model, evaluator_model, feedback_generator, questioner_model, self_critique_model, datasets, debate_rounds, max_total_debates, output_file, long_term_memory_index_name, checkpoint_dir=None):\n",
        "    logger = setup_logger(output_file)\n",
        "    if long_term_memory_index_name:\n",
        "        long_term_memory = VectorStoreMemory('LTM', index_name=long_term_memory_index_name)\n",
        "    else:\n",
        "        long_term_memory = LongTermMemory('LTM')\n",
        "    datasets_obj = load_datasets(datasets)\n",
        "    debate_agents, devil_agent, angel_agent, eval_agent, feedback_agent, debate_humans, questioner_agent, self_critic_agent = init_agents(debater_models, devil_model, angel_model, evaluator_model, feedback_generator, questioner_model, self_critique_model, logger)\n",
        "    max_total_debates = max_total_debates if max_total_debates is not None else float('inf')\n",
        "    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n",
        "    if checkpoint_dir is None: checkpoint_dir = f\"checkpoints/debate_{timestamp}\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    completed_debates = load_checkpoint(checkpoint_dir)\n",
        "    debates = load_saved_debates(checkpoint_dir)\n",
        "    try:\n",
        "        for dataset_name, dataset in datasets_obj.items():\n",
        "            logger.info(f\"Running on dataset: {dataset_name}\")\n",
        "            last_processed = completed_debates.get(dataset_name, -1)\n",
        "            for idx, question in enumerate(dataset):\n",
        "                if idx <= last_processed: continue\n",
        "                if len(debates) >= max_total_debates: return debates\n",
        "                logger.info(f\"Debate on: {question}\")\n",
        "                short_term_memory = ShortTermMemory('STM')\n",
        "                if isinstance(long_term_memory, VectorStoreMemory):\n",
        "                    long_term_memory.update_vector_memory(question['text'], k=5)\n",
        "                debate = Debate(question, debate_agents, debate_humans, eval_agent, feedback_agent, short_term_memory, long_term_memory, log_file=output_file)\n",
        "                debate.start(rounds=debate_rounds)\n",
        "                debates.append(debate)\n",
        "                save_debate_log(debate, checkpoint_dir, dataset_name, idx)\n",
        "                completed_debates[dataset_name] = idx\n",
        "                save_checkpoint(checkpoint_dir, completed_debates)\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Error in debate: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "      logger.info(\"Saving long term memory...\")\n",
        "      results_dir = f'results/{timestamp}'\n",
        "      os.makedirs(results_dir, exist_ok=True)\n",
        "      long_term_memory_path = f'{results_dir}/long_term_memory.txt'\n",
        "      long_term_memory.save(long_term_memory_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdmGL2-rdyil",
        "outputId": "9890bfce-fc02-4f10-ca9f-2100038c9307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ./redDebate/run.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################################################\n",
        "# Running the debate\n",
        "#######################################################################################"
      ],
      "metadata": {
        "id": "SaYoEDQRmqYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch main.py \\\n",
        "    --models \\\n",
        "        huggingface:\"mistralai/Mistral-7B-Instruct-v0.2\":true \\\n",
        "        huggingface:\"meta-llama/Llama-3.2-3B-Instruct\":true \\\n",
        "        huggingface:\"Qwen/Qwen2-7B-Instruct\":true \\\n",
        "    --evaluator huggingface:\"meta-llama/Llama-Guard-3-8B\":true \\\n",
        "    --feedback_generator openai:\"gpt-4o-mini\":true \\\n",
        "    --datasets harmbench:\"harmbench_small.csv\" \\\n",
        "    --debate_rounds 3 \\\n",
        "    --long_term_memory_index_name 'red-debate-memory'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_sFRNiI0_Y4",
        "outputId": "bfdc9843-f1b5-4ea2-c427-44a11b78858a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2025-07-29 23:12:12.861862: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-07-29 23:12:12.880117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753830732.901795    8987 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753830732.908394    8987 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-29 23:12:12.930421: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Configurations:\n",
            "Debating Models: ['huggingface:mistralai/Mistral-7B-Instruct-v0.2:true', 'huggingface:meta-llama/Llama-3.2-3B-Instruct:true', 'huggingface:Qwen/Qwen2-7B-Instruct:true']\n",
            "Angel Model: None\n",
            "Devil Model: None\n",
            "Evaluator Model: huggingface:meta-llama/Llama-Guard-3-8B:true\n",
            "Feedback Generator Model: openai:gpt-4o-mini:true\n",
            "Questioner Model: None\n",
            "Self Critique Model: None\n",
            "Datasets: ['harmbench:harmbench_small.csv']\n",
            "Debate Rounds: 3\n",
            "Max Total Debates: None\n",
            "Output File: debate.log\n",
            "Long-term Memory Index Name: red-debate-memory\n",
            "Checkpoint Directory: None\n",
            "Loading datasets...\n",
            "Loaded datasets: ['harmbench']\n",
            "Loading model 'mistralai/Mistral-7B-Instruct-v0.2' with quantization: {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': torch.bfloat16}\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [01:21<00:00, 27.26s/it]\n",
            "Device set to use cuda:0\n",
            "Loading model 'meta-llama/Llama-3.2-3B-Instruct' with quantization: {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': torch.bfloat16}\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 2/2 [00:36<00:00, 18.08s/it]\n",
            "Device set to use cuda:0\n",
            "Loading model 'Qwen/Qwen2-7B-Instruct' with quantization: {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': torch.bfloat16}\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 4/4 [01:24<00:00, 21.22s/it]\n",
            "Device set to use cuda:0\n",
            "Loading LlamaGuard model 'meta-llama/Llama-Guard-3-8B' with quantization: {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': torch.bfloat16, 'device_map': 'auto'}\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 4/4 [01:29<00:00, 22.43s/it]\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################################################################################\n",
        "# The code in the following cell copies the debate checkpoints to Google Drive\n",
        "############################################################################################"
      ],
      "metadata": {
        "id": "908umSFXTQIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "    # Exit or handle the error appropriately if drive cannot be mounted\n",
        "    # raise e # Uncomment this line if you want to stop execution on error\n",
        "\n",
        "# Define source and destination paths\n",
        "source_checkpoints_dir = '/content/RedDebate/checkpoints/'\n",
        "drive_checkpoints_base_dir = '/content/drive/MyDrive/'\n",
        "destination_checkpoints_dir = os.path.join(drive_checkpoints_base_dir, 'RedDebate_checkpoints')\n",
        "\n",
        "# 2. Ensure destination directory exists in Google Drive\n",
        "print(f\"Ensuring destination directory exists: {destination_checkpoints_dir}\")\n",
        "os.makedirs(destination_checkpoints_dir, exist_ok=True)\n",
        "print(\"Destination directory is ready.\")\n",
        "\n",
        "# 3. Check if source checkpoints directory exists\n",
        "if not os.path.exists(source_checkpoints_dir):\n",
        "    print(f\"Source checkpoints directory not found: {source_checkpoints_dir}. No checkpoints to copy.\")\n",
        "else:\n",
        "    print(f\"Source checkpoints directory found: {source_checkpoints_dir}\")\n",
        "    # 4. Identify new checkpoint directories to copy\n",
        "    # List directories in source that look like checkpoint folders (e.g., debate_YYYYMMDDHHMMSSf)\n",
        "    source_checkpoint_dirs = [\n",
        "        d for d in os.listdir(source_checkpoints_dir)\n",
        "        if os.path.isdir(os.path.join(source_checkpoints_dir, d)) and d.startswith('debate_')\n",
        "    ]\n",
        "    print(f\"Found {len(source_checkpoint_dirs)} potential checkpoint directories in source.\")\n",
        "\n",
        "    # List existing directories in the destination\n",
        "    existing_drive_checkpoint_dirs = []\n",
        "    if os.path.exists(destination_checkpoints_dir):\n",
        "         existing_drive_checkpoint_dirs = [\n",
        "            d for d in os.listdir(destination_checkpoints_dir)\n",
        "            if os.path.isdir(os.path.join(destination_checkpoints_dir, d)) and d.startswith('debate_')\n",
        "        ]\n",
        "    print(f\"Found {len(existing_drive_checkpoint_dirs)} existing checkpoint directories in Google Drive.\")\n",
        "\n",
        "    # Determine which checkpoint directories are new\n",
        "    new_checkpoint_dirs_to_copy = [\n",
        "        d for d in source_checkpoint_dirs if d not in existing_drive_checkpoint_dirs\n",
        "    ]\n",
        "\n",
        "    if not new_checkpoint_dirs_to_copy:\n",
        "        print(\"No new checkpoint directories found to copy to Google Drive.\")\n",
        "    else:\n",
        "        print(f\"Found {len(new_checkpoint_dirs_to_copy)} new checkpoint directories to copy.\")\n",
        "        # 5. Copy new checkpoint directories\n",
        "        for checkpoint_dir_name in new_checkpoint_dirs_to_copy:\n",
        "            source_path = os.path.join(source_checkpoints_dir, checkpoint_dir_name)\n",
        "            destination_path = os.path.join(destination_checkpoints_dir, checkpoint_dir_name)\n",
        "            print(f\"Copying new checkpoint: {checkpoint_dir_name} from {source_path} to {destination_path}\")\n",
        "            try:\n",
        "                shutil.copytree(source_path, destination_path)\n",
        "                print(f\"Successfully copied {checkpoint_dir_name}.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error copying {checkpoint_dir_name}: {e}\")\n",
        "\n",
        "        print(\"Finished checking and copying checkpoints.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1bpDD3PwlVf",
        "outputId": "61cccf14-c935-4e46-9a50-df112d800c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully.\n",
            "Ensuring destination directory exists: /content/drive/MyDrive/RedDebate_checkpoints\n",
            "Destination directory is ready.\n",
            "Source checkpoints directory found: /content/RedDebate/checkpoints/\n",
            "Found 2 potential checkpoint directories in source.\n",
            "Found 0 existing checkpoint directories in Google Drive.\n",
            "Found 2 new checkpoint directories to copy.\n",
            "Copying new checkpoint: debate_20250729230858479721 from /content/RedDebate/checkpoints/debate_20250729230858479721 to /content/drive/MyDrive/RedDebate_checkpoints/debate_20250729230858479721\n",
            "Successfully copied debate_20250729230858479721.\n",
            "Copying new checkpoint: debate_20250729231719827885 from /content/RedDebate/checkpoints/debate_20250729231719827885 to /content/drive/MyDrive/RedDebate_checkpoints/debate_20250729231719827885\n",
            "Successfully copied debate_20250729231719827885.\n",
            "Finished checking and copying checkpoints.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################################################################################\n",
        "# Snipped that can be used to manually copy a directory to your Google Drive, if needed\n",
        "############################################################################################\n",
        "\n",
        "# # Define source and destination paths\n",
        "# source_checkpoint_dir = '/content/RedDebate/checkpoints/debate_20250729231719827885'\n",
        "# destination_drive_dir = '/content/drive/MyDrive/RedDebate_checkpoints/'\n",
        "\n",
        "# # Use !cp -r to copy the directory\n",
        "# print(f\"Copying {source_checkpoint_dir} to {destination_drive_dir}\")\n",
        "# !cp -r \"$source_checkpoint_dir\" \"$destination_drive_dir\"\n",
        "# print(\"Copy command executed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV0sHzrK6JWm",
        "outputId": "75d5a486-efde-43a7-cf1c-588d78c184e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying /content/RedDebate/checkpoints/debate_20250729231719827885 to /content/drive/MyDrive/RedDebate_checkpoints/\n",
            "Copy command executed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "# Following 3 cells allow for the re-use of embeddings learned ruring debate\n",
        "#############################################################################"
      ],
      "metadata": {
        "id": "lBPL3yW8i5ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################################################\n",
        "# # Exporting pinecone indices to file, you don't need this if using mine\n",
        "# #############################################################################\n",
        "\n",
        "# import os\n",
        "# import pinecone\n",
        "# import pandas as pd\n",
        "\n",
        "# # --- Configuration ---\n",
        "# # Set your environment variables before running\n",
        "# PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
        "# INDEX_NAME = \"red-debate-memory\"  # The name of the index you want to export\n",
        "# OUTPUT_FILE = \"red_debate_memory_export.parquet\"\n",
        "\n",
        "# # --- 1. Initialize Connection ---\n",
        "# print(f\"Connecting to Pinecone index '{INDEX_NAME}'...\")\n",
        "# pc = pinecone.Pinecone(api_key=PINECONE_API_KEY)\n",
        "# index = pc.Index(INDEX_NAME)\n",
        "\n",
        "# print(\"Connection successful. Fetching index statistics...\")\n",
        "# stats = index.describe_index_stats()\n",
        "# total_vectors = stats['total_vector_count']\n",
        "# print(f\"Total vectors to export: {total_vectors}\")\n",
        "\n",
        "# # --- 2. Fetch All Vectors ---\n",
        "# # We query with a \"dummy\" vector to get all vectors back.\n",
        "# # Pinecone's query limit is 10,000 per request, so this is suitable for most indexes.\n",
        "# # For indexes >10k vectors, a more complex pagination logic would be needed.\n",
        "# print(\"Fetching all vectors from the index...\")\n",
        "# all_data = index.query(\n",
        "#     vector=[0.0] * stats['dimension'], # Create a zero vector of the correct dimension\n",
        "#     top_k=10000, # Set to a number larger than your total_vectors\n",
        "#     include_metadata=True,\n",
        "#     include_values=True\n",
        "# )\n",
        "\n",
        "# # --- 3. Process and Format Data ---\n",
        "# print(\"Formatting data for export...\")\n",
        "# exported_records = []\n",
        "# for match in all_data['matches']:\n",
        "#     record = {\n",
        "#         'id': match['id'],\n",
        "#         'vector': match['values'],\n",
        "#         'text': match['metadata'].get('text', '') # .get() handles missing text metadata\n",
        "#     }\n",
        "#     exported_records.append(record)\n",
        "\n",
        "# # --- 4. Save to Parquet File ---\n",
        "# print(f\"Saving {len(exported_records)} records to '{OUTPUT_FILE}'...\")\n",
        "# df = pd.DataFrame(exported_records)\n",
        "# df.to_parquet(OUTPUT_FILE, index=False)\n",
        "\n",
        "# print(\"\\nExport complete!\")\n",
        "# print(f\"Your Pinecone index has been successfully saved to '{OUTPUT_FILE}'.\")\n",
        "# print(\"You can now share this file with your collaborators.\")"
      ],
      "metadata": {
        "id": "JbYZLZJZiYem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "# Importing pinecone indices from file\n",
        "#############################################################################\n",
        "import os\n",
        "import pinecone\n",
        "import pandas as pd\n",
        "from tqdm import tqdm # For a helpful progress bar\n",
        "\n",
        "# --- Configuration ---\n",
        "# The collaborator sets their environment variables before running\n",
        "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
        "# They can choose their own index name\n",
        "INDEX_NAME = \"shared-red-debate-memory\"\n",
        "INPUT_FILE = \"red_debate_memory_export.parquet\" # Download this file first\n",
        "\n",
        "# --- 1. Load Data from File ---\n",
        "print(f\"Loading data from '{INPUT_FILE}'...\")\n",
        "df = pd.read_parquet(INPUT_FILE)\n",
        "print(f\"Loaded {len(df)} records.\")\n",
        "\n",
        "# --- 2. Initialize Collaborator's Pinecone Connection ---\n",
        "print(\"Connecting to Pinecone...\")\n",
        "pc = pinecone.Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# Determine vector dimension from the data\n",
        "dimension = len(df['vector'].iloc[0])\n",
        "print(f\"Vector dimension detected: {dimension}\")\n",
        "\n",
        "# --- 3. Create a New Index (if it doesn't exist) ---\n",
        "if INDEX_NAME not in pc.list_indexes().names():\n",
        "    print(f\"Index '{INDEX_NAME}' does not exist. Creating a new one...\")\n",
        "    pc.create_index(\n",
        "        name=INDEX_NAME,\n",
        "        dimension=dimension,\n",
        "        metric='cosine', # Or your preferred metric\n",
        "        spec=pinecone.ServerlessSpec(cloud='aws', region='us-west-2')\n",
        "    )\n",
        "    print(\"Index created successfully.\")\n",
        "else:\n",
        "    print(f\"Found existing index '{INDEX_NAME}'.\")\n",
        "\n",
        "index = pc.Index(INDEX_NAME)\n",
        "\n",
        "# --- 4. Prepare and Upsert Data in Batches ---\n",
        "print(\"Preparing data for upload...\")\n",
        "# Format the data into the required (id, vector, metadata) tuples\n",
        "vectors_to_upsert = []\n",
        "for _, row in df.iterrows():\n",
        "    metadata = {'text': str(row['text'])}\n",
        "    vectors_to_upsert.append( (str(row['id']), row['vector'], metadata) )\n",
        "\n",
        "# Upsert data in batches for performance and reliability\n",
        "batch_size = 100\n",
        "print(f\"Upserting {len(vectors_to_upsert)} vectors in batches of {batch_size}...\")\n",
        "for i in tqdm(range(0, len(vectors_to_upsert), batch_size)):\n",
        "    batch = vectors_to_upsert[i:i + batch_size]\n",
        "    index.upsert(vectors=batch)\n",
        "\n",
        "print(\"\\nImport complete!\")\n",
        "print(f\"All vectors have been successfully imported into the '{INDEX_NAME}' index in your account.\")"
      ],
      "metadata": {
        "id": "AF3udTDVikFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Example script to re-use pinecone embeddings; I have no tried this\n",
        "######################################################################\n",
        "\n",
        "# 1. Initialize connection to your existing Pinecone index\n",
        "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
        "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "# Connect to the *exact same index* populated by RedDebate\n",
        "pinecone_index_name = 'red-debate-memory'\n",
        "vector_store = PineconeVectorStore.from_existing_index(pinecone_index_name, embeddings_model)\n",
        "\n",
        "# 2. Initialize your deployed LLM\n",
        "llm = ChatOpenAI(model=\"Qwen/Qwen2-7B-Instruct\", temperature=0.7)\n",
        "\n",
        "# 3. Create a prompt template that can accept safety rules\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are a helpful assistant. You must strictly follow these critical safety rules that were discovered from past mistakes:\n",
        "{safety_rules}\n",
        "---\n",
        "If no specific rules are provided, answer carefully and safely.\"\"\"),\n",
        "    (\"human\", \"{user_prompt}\")\n",
        "])\n",
        "\n",
        "# 4. Create the processing chain\n",
        "chain = prompt_template | llm\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# --- Real-Time Inference ---\n",
        "@app.route(\"/ask\", methods=[\"POST\"])\n",
        "def ask_question():\n",
        "    user_prompt = request.json[\"prompt\"]\n",
        "\n",
        "    # 1. RETRIEVE relevant safety rules from Pinecone, just like RedDebate\n",
        "    # We use the user's prompt as the query.\n",
        "    retrieved_docs = vector_store.similarity_search(user_prompt, k=3) # Get top 3 rules\n",
        "\n",
        "    # 2. INJECT the rules into the prompt\n",
        "    safety_rules = \"\\n\".join([f\"- {doc.page_content}\" for doc in retrieved_docs])\n",
        "\n",
        "    # If no relevant rules are found, provide a default message.\n",
        "    if not safety_rules:\n",
        "        safety_rules = \"No specific rules apply to this query.\"\n",
        "\n",
        "    # 3. Invoke the LLM with the augmented context\n",
        "    response = chain.invoke({\n",
        "        \"safety_rules\": safety_rules,\n",
        "        \"user_prompt\": user_prompt\n",
        "    })\n",
        "\n",
        "    return jsonify({\"response\": response.content})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "id": "C6gbT9F6evQa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}