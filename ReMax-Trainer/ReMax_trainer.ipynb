{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Complete ReMax Training (3 Epochs / KL divergence / BF16)\n",
        "'''\n",
        "=====================================================================================================\n",
        "This training script was originally developed and optimized for execution within Google Colab,\n",
        "relying heavily on Google Drive for persistent storage, Colab-specific authentication mechanisms,\n",
        "and other environment-dependent utilities. As a result, the initial implementation included\n",
        "Drive-mounted checkpoint directories, CSV logging to Drive, and secret-based Hugging Face login via\n",
        "Colab‚Äôs userdata API. While these components streamlined experimentation within a Colab workflow,\n",
        "they also made the script less portable and harder to reproduce in general compute environments\n",
        "such as local machines, cloud VMs, or managed training clusters.\n",
        "\n",
        "You can refactor the current version and remove the above mentioned Colab-specific assumptions,\n",
        "replacing them with environment-agnostic paths, standard Hugging Face authentication, and fully\n",
        "general dataset/model loading logic so the script can run consistently anywhere while retaining\n",
        "the same behavior and training methodology.\n",
        "=====================================================================================================\n",
        "'''\n",
        "# ==========================================\n",
        "# 1. Cleanup & Install\n",
        "# ==========================================\n",
        "\n",
        "print(\"‚è≥ Installing libraries...\")\n",
        "!pip install -q -U transformers datasets accelerate huggingface_hub bitsandbytes\n",
        "'''\n",
        "The training was conducted using the following library versions at the time:\n",
        "Accelerate: 0.28.0\n",
        "Hugging Face Hub: 0.17.1\n",
        "Transformers: 4.57.3\n",
        "Pytorch: 2.9.0+cu126\n",
        "Datasets: 4.4.1\n",
        "Tokenizers: 0.22.1\n",
        "Bitsandbytes: 0.48.2 (it was used as everything here was done in BF16, SFT and RM were loaded in BF16)\n",
        "\n",
        "NOTE : TRL didn't had a drop in for ReMax method at the time so this is a custom raw implementation of ReMax\n",
        "'''\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import csv\n",
        "import math\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    get_scheduler\n",
        ")\n",
        "from huggingface_hub import login, HfApi\n",
        "from google.colab import userdata, drive\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ==========================================\n",
        "# 2. Setup & Login\n",
        "# ==========================================\n",
        "print(\"\\nüìÇ Mounting Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define Paths\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/Qwen3-ReMax-Training\"\n",
        "LOG_FILE = f\"{DRIVE_ROOT}/remax_logs.csv\"\n",
        "CHECKPOINT_DIR = f\"{DRIVE_ROOT}/checkpoints\"\n",
        "EPOCH_DIR = f\"{DRIVE_ROOT}/epoch_saves\"\n",
        "\n",
        "for d in [CHECKPOINT_DIR, EPOCH_DIR]:\n",
        "# Create directories and initialize the CSV log header if missing\n",
        "    if not os.path.exists(d):\n",
        "        os.makedirs(d)\n",
        "\n",
        "# Init Log File (Added 'Phase' column for epochs)\n",
        "if not os.path.exists(LOG_FILE):\n",
        "    with open(LOG_FILE, 'w') as f:\n",
        "        f.write(\"Global_Step,Epoch,Phase,Loss,Advantage,Reward_Sample,Reward_Greedy,KL_Proxy\\n\")\n",
        "\n",
        "# Hugging Face Login\n",
        "print(\"\\nüîë Logging in...\")\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    login(token=hf_token, add_to_git_credential=True)\n",
        "    print(\"‚úÖ Logged in via Colab Secret.\")\n",
        "except:\n",
        "  print(\"‚ö†Ô∏è Secret 'HF_TOKEN' not found. Falling back to manual input.\")\n",
        "  login(add_to_git_credential=True)\n",
        "\n",
        "# ==========================================\n",
        "# 3. Configuration\n",
        "# ==========================================\n",
        "SFT_MODEL_ID = \"AIPlans/qwen3-0.6b-SFT-hs2\" # this works as policy(to be trained) and reference model both\n",
        "RM_MODEL_ID  = \"AIPlans/qwen3-0.6b-RM-hs2\"\n",
        "OUTPUT_REPO  = \"your-username/qwen3-0.6b-ReMax\" # this naming is arbitrary\n",
        "DATASET_NAME = \"Jennny/helpsteer2-helpfulness-preference\" # this is a variant of the HelpSteer2 dataset having only the helpfulness attribute\n",
        "DEVICE = \"cuda\"\n",
        "\n",
        "# All hyperparameters can be modified as suitable (A100 80GB was used at the time)\n",
        "LR = 5e-7 # a lower learning rate as there are 3 epochs\n",
        "BETA = 0.1 # for KL divergence\n",
        "BATCH_SIZE = 16\n",
        "GRAD_ACCUMULATION = 2     # Effective Batch = 32\n",
        "EPOCHS = 3\n",
        "MAX_NEW_TOKENS = 128\n",
        "MAX_PROMPT_LENGTH = 1024\n",
        "LOGGING_STEPS = 20        # Log every 20 steps\n",
        "\n",
        "# ==========================================\n",
        "# 4. Load Models (BF16)\n",
        "# ==========================================\n",
        "print(\"\\nüß† Loading Models...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL_ID, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# 1. Actor (Trainable)\n",
        "actor = AutoModelForCausalLM.from_pretrained(\n",
        "    SFT_MODEL_ID, dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True\n",
        ")\n",
        "actor.config.use_cache = False\n",
        "\n",
        "# 2. Reference (Frozen weights)\n",
        "ref = AutoModelForCausalLM.from_pretrained(\n",
        "    SFT_MODEL_ID, dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True\n",
        ")\n",
        "ref.eval()\n",
        "\n",
        "# 3. Reward (Frozen weights)\n",
        "rm = AutoModelForSequenceClassification.from_pretrained(\n",
        "    RM_MODEL_ID, num_labels=1, dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True\n",
        ")\n",
        "rm.eval()\n",
        "\n",
        "print(\"‚úÖ Models Loaded.\")\n",
        "\n",
        "# ==========================================\n",
        "# 5. Dataset (Train/Val Split)\n",
        "# ==========================================\n",
        "print(f\"Loading and Preparing dataset: {DATASET_NAME}...\")\n",
        "# Load and Filter\n",
        "full_dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
        "full_dataset = full_dataset.filter(lambda x: x[\"chosen_score\"] >= 3)\n",
        "\n",
        "# Split: 95% Train, 5% Validation\n",
        "split_dataset = full_dataset.train_test_split(test_size=0.05, seed=42)\n",
        "train_data = split_dataset['train']\n",
        "val_data = split_dataset['test'].select(range(64)) # Keep validation fast (64 examples)\n",
        "\n",
        "def preprocess(examples):\n",
        "    prompts = []\n",
        "    for chosen in examples[\"chosen\"]:\n",
        "        if isinstance(chosen, list): prompt = chosen[0]['content']\n",
        "        else: prompt = str(chosen).split(\"Assistant:\")[0]\n",
        "        prompts.append(f\"User: {prompt}\\n\\nAssistant:\")\n",
        "    return tokenizer(prompts, padding=\"max_length\", truncation=True, max_length=MAX_PROMPT_LENGTH, return_tensors=\"pt\")\n",
        "\n",
        "# Process Train\n",
        "train_dataset = train_data.map(preprocess, batched=True, remove_columns=train_data.column_names)\n",
        "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Process Val\n",
        "val_dataset = val_data.map(preprocess, batched=True, remove_columns=val_data.column_names)\n",
        "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"‚úÖ Dataset Split: {len(train_dataset)} Train | {len(val_dataset)} Validation\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. Optimizer\n",
        "# ==========================================\n",
        "optimizer = AdamW(actor.parameters(), lr=LR)\n",
        "num_training_steps = math.ceil(len(dataloader) * EPOCHS / GRAD_ACCUMULATION)\n",
        "lr_scheduler = get_scheduler(\"cosine\", optimizer=optimizer, num_warmup_steps=20, num_training_steps=num_training_steps)\n",
        "\n",
        "# ==========================================\n",
        "# 7. Helper Functions\n",
        "# ==========================================\n",
        "def get_log_probs(model, input_ids, attention_mask):\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits[:, :-1, :]\n",
        "    labels = input_ids[:, 1:]\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    return torch.gather(log_probs, -1, labels.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "def get_reward(texts):\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        return rm(**inputs).logits.squeeze(-1)\n",
        "\n",
        "def run_evaluation(epoch, global_step):\n",
        "    print(f\"\\nüîç Running Validation (Epoch {epoch+1})...\")\n",
        "    actor.eval()\n",
        "\n",
        "    val_loss, val_adv, val_r_sample, val_r_greedy, val_kl = 0, 0, 0, 0, 0\n",
        "    steps = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            # --- Generation ---\n",
        "            prompt_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "            prompt_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "\n",
        "            sample_out = actor.generate(prompt_ids, attention_mask=prompt_mask, max_new_tokens=MAX_NEW_TOKENS, do_sample=True, top_p=0.9, temperature=1.0, pad_token_id=tokenizer.pad_token_id)\n",
        "            greedy_out = actor.generate(prompt_ids, attention_mask=prompt_mask, max_new_tokens=MAX_NEW_TOKENS, do_sample=False, pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "            # --- Rewards ---\n",
        "            actor_device = next(actor.parameters()).device\n",
        "            rm_device = next(rm.parameters()).device\n",
        "\n",
        "            sample_mask = (sample_out != tokenizer.pad_token_id).long()\n",
        "            greedy_mask = (greedy_out != tokenizer.pad_token_id).long()\n",
        "\n",
        "            r_sample = rm(input_ids=sample_out.to(rm_device), attention_mask=sample_mask.to(rm_device)).logits.squeeze(-1).to(actor_device).to(torch.bfloat16)\n",
        "            r_greedy = rm(input_ids=greedy_out.to(rm_device), attention_mask=greedy_mask.to(rm_device)).logits.squeeze(-1).to(actor_device).to(torch.bfloat16)\n",
        "\n",
        "            # --- KL & Loss (For metrics only) ---\n",
        "            sample_out = sample_out.to(actor_device)\n",
        "            sample_mask = sample_mask.to(actor_device)\n",
        "\n",
        "            log_probs_actor = get_log_probs(actor, sample_out, sample_mask)\n",
        "            log_probs_ref = get_log_probs(ref, sample_out, sample_mask)\n",
        "\n",
        "            start = prompt_ids.shape[1] - 1\n",
        "            min_len = min(log_probs_actor.shape[1], log_probs_ref.shape[1])\n",
        "\n",
        "            if start < min_len:\n",
        "                logp_gen = log_probs_actor[:, start:min_len]\n",
        "                valid_mask = sample_mask[:, start+1 : start+1 + logp_gen.shape[1]].to(actor_device).to(torch.bfloat16)\n",
        "\n",
        "                logp_sum = (logp_gen * valid_mask).sum(dim=-1)\n",
        "\n",
        "                # KL Proxy\n",
        "                ref_gen = log_probs_ref[:, start:min_len]\n",
        "                kl_sum = ((logp_gen - ref_gen) * valid_mask).sum(dim=-1)\n",
        "                num_tokens = valid_mask.sum(dim=-1).clamp(min=1.0)\n",
        "                kl_div_normalized = kl_sum / num_tokens\n",
        "\n",
        "                advantage = (r_sample - BETA * kl_div_normalized) - r_greedy\n",
        "                loss = - (logp_sum * advantage).mean()\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_adv += advantage.mean().item()\n",
        "                val_r_sample += r_sample.mean().item()\n",
        "                val_r_greedy += r_greedy.mean().item()\n",
        "                val_kl += kl_div_normalized.mean().item()\n",
        "                steps += 1\n",
        "\n",
        "    # Average metrics\n",
        "    if steps > 0:\n",
        "        val_loss /= steps\n",
        "        val_adv /= steps\n",
        "        val_r_sample /= steps\n",
        "        val_r_greedy /= steps\n",
        "        val_kl /= steps\n",
        "\n",
        "    print(f\"üìä Validation Results: Loss: {val_loss:.4f} | Adv: {val_adv:.4f} | R_Sample: {val_r_sample:.4f}\")\n",
        "\n",
        "    # Log Validation Row\n",
        "    with open(LOG_FILE, 'a') as f:\n",
        "        f.write(f\"{global_step},{epoch+1},Val,{val_loss:.4f},{val_adv:.4f},{val_r_sample:.4f},{val_r_greedy:.4f},{val_kl:.4f}\\n\")\n",
        "\n",
        "    actor.train()\n",
        "\n",
        "# ==========================================\n",
        "# 8. Training Loop\n",
        "# ==========================================\n",
        "print(\"\\nüöÄ Starting ReMax Loop...\")\n",
        "actor.train()\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "optimizer.zero_grad()\n",
        "\n",
        "acc_loss, acc_adv, acc_r_sample, acc_r_greedy, acc_kl = 0, 0, 0, 0, 0\n",
        "global_step = 0\n",
        "\n",
        "# epochs were run in phases\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n--- Starting Epoch {epoch+1}/{EPOCHS} ---\")\n",
        "\n",
        "    for step, batch in enumerate(dataloader):\n",
        "\n",
        "        # --- A. Generation ---\n",
        "        prompt_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "        prompt_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sample_out = actor.generate(\n",
        "                prompt_ids, attention_mask=prompt_mask, max_new_tokens=MAX_NEW_TOKENS,\n",
        "                do_sample=True, top_p=0.9, temperature=1.0, pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "            greedy_out = actor.generate(\n",
        "                prompt_ids, attention_mask=prompt_mask, max_new_tokens=MAX_NEW_TOKENS,\n",
        "                do_sample=False, pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "        # --- B. Rewards ---\n",
        "        actor_device = next(actor.parameters()).device\n",
        "        rm_device = next(rm.parameters()).device\n",
        "\n",
        "        sample_mask = (sample_out != tokenizer.pad_token_id).long()\n",
        "        greedy_mask = (greedy_out != tokenizer.pad_token_id).long()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            r_sample = rm(input_ids=sample_out.to(rm_device), attention_mask=sample_mask.to(rm_device)).logits.squeeze(-1)\n",
        "            r_greedy = rm(input_ids=greedy_out.to(rm_device), attention_mask=greedy_mask.to(rm_device)).logits.squeeze(-1)\n",
        "\n",
        "        r_sample = r_sample.to(actor_device).to(torch.bfloat16)\n",
        "        r_greedy = r_greedy.to(actor_device).to(torch.bfloat16)\n",
        "\n",
        "        # --- C. KL & Log Probs ---\n",
        "        sample_out = sample_out.to(actor_device)\n",
        "        sample_mask = sample_mask.to(actor_device)\n",
        "\n",
        "        log_probs_actor = get_log_probs(actor, sample_out, sample_mask)\n",
        "        with torch.no_grad():\n",
        "            log_probs_ref = get_log_probs(ref, sample_out, sample_mask)\n",
        "\n",
        "        prompt_len = prompt_ids.shape[1]\n",
        "        start = prompt_len - 1\n",
        "        min_len = min(log_probs_actor.shape[1], log_probs_ref.shape[1])\n",
        "\n",
        "        if start >= min_len:\n",
        "            logp_sum = torch.zeros(prompt_ids.size(0), device=actor_device)\n",
        "            kl_div_normalized = torch.zeros_like(logp_sum)\n",
        "        else:\n",
        "            logp_gen = log_probs_actor[:, start:min_len]\n",
        "            valid_mask = sample_mask[:, start+1 : start+1 + logp_gen.shape[1]]\n",
        "            valid_mask = valid_mask[:, :logp_gen.shape[1]].to(actor_device)\n",
        "\n",
        "            valid_mask_f = valid_mask.to(torch.bfloat16)\n",
        "            logp_gen_f = logp_gen.to(torch.bfloat16)\n",
        "\n",
        "            logp_masked = logp_gen_f * valid_mask_f\n",
        "            logp_sum = logp_masked.sum(dim=-1)\n",
        "\n",
        "            # KL Proxy Calculation\n",
        "            ref_gen_f = log_probs_ref[:, start:min_len].to(torch.bfloat16)\n",
        "            kl_gen = (logp_gen_f - ref_gen_f) * valid_mask_f\n",
        "            kl_sum = kl_gen.sum(dim=-1)\n",
        "            num_tokens = valid_mask_f.sum(dim=-1).clamp(min=1.0)\n",
        "            kl_div_normalized = kl_sum / num_tokens\n",
        "\n",
        "        # --- D. Loss Calculation ---\n",
        "        advantage = (r_sample - BETA * kl_div_normalized) - r_greedy\n",
        "        advantage = advantage.clamp(-10.0, 10.0)\n",
        "\n",
        "        loss = - (logp_sum * advantage.detach()).mean()\n",
        "        loss = loss / GRAD_ACCUMULATION\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        acc_loss += loss.item() * GRAD_ACCUMULATION\n",
        "        acc_adv += advantage.mean().item()\n",
        "        acc_r_sample += r_sample.mean().item()\n",
        "        acc_r_greedy += r_greedy.mean().item()\n",
        "        acc_kl += kl_div_normalized.mean().item()\n",
        "\n",
        "        # --- E. Optimizer Step ---\n",
        "        if (step + 1) % GRAD_ACCUMULATION == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(actor.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            progress_bar.update(1)\n",
        "            global_step += 1\n",
        "\n",
        "            # Log Training\n",
        "            if global_step % LOGGING_STEPS == 0:\n",
        "                with open(LOG_FILE, 'a') as f:\n",
        "                    f.write(f\"{global_step},{epoch+1},Train,{acc_loss/GRAD_ACCUMULATION},{acc_adv/GRAD_ACCUMULATION},{acc_r_sample/GRAD_ACCUMULATION},{acc_r_greedy/GRAD_ACCUMULATION},{acc_kl/GRAD_ACCUMULATION}\\n\")\n",
        "\n",
        "                print(f\"Step {global_step} | Loss: {acc_loss/GRAD_ACCUMULATION:.4f} | Adv: {acc_adv/GRAD_ACCUMULATION:.4f} | R_Sample: {acc_r_sample/GRAD_ACCUMULATION:.4f}\")\n",
        "\n",
        "            acc_loss, acc_adv, acc_r_sample, acc_r_greedy, acc_kl = 0, 0, 0, 0, 0\n",
        "\n",
        "            # Checkpoint Rotation for storage efficiency in drive\n",
        "            if global_step > 0 and global_step % 50 == 0:\n",
        "                ckpt_path = f\"{CHECKPOINT_DIR}/step_{global_step}\"\n",
        "                actor.save_pretrained(ckpt_path)\n",
        "                tokenizer.save_pretrained(ckpt_path)\n",
        "                print(f\"üíæ Checkpoint saved: {ckpt_path}\")\n",
        "\n",
        "                all_ckpts = sorted([os.path.join(CHECKPOINT_DIR, d) for d in os.listdir(CHECKPOINT_DIR) if d.startswith(\"step_\")], key=os.path.getmtime)\n",
        "                if len(all_ckpts) > 2:\n",
        "                    for old_ckpt in all_ckpts[:-2]:\n",
        "                        shutil.rmtree(old_ckpt)\n",
        "\n",
        "    # --- End of Epoch Validation & Save ---\n",
        "    run_evaluation(epoch, global_step) # Run Validation Suite\n",
        "\n",
        "    epoch_save_path = f\"{EPOCH_DIR}/epoch_{epoch+1}\"\n",
        "    print(f\"üéâ Epoch {epoch+1} Complete! Saving to {epoch_save_path}...\")\n",
        "    actor.save_pretrained(epoch_save_path)\n",
        "    tokenizer.save_pretrained(epoch_save_path)\n",
        "\n",
        "# ==========================================\n",
        "# 9. Final Save & Push\n",
        "# ==========================================\n",
        "print(\"\\n‚òÅÔ∏è Pushing BF16 Model to Hub...\")\n",
        "api = HfApi()\n",
        "api.create_repo(repo_id=OUTPUT_REPO, exist_ok=True)\n",
        "actor.push_to_hub(OUTPUT_REPO)\n",
        "tokenizer.push_to_hub(OUTPUT_REPO)\n",
        "\n",
        "print(f\"‚úÖ ReMax Training Complete! Model: https://huggingface.co/{OUTPUT_REPO}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WswDrJKIfKA3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}