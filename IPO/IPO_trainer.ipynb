{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Complete IPO Training (3 epochs / Beta 0.01 / BF16)\n",
        "'''\n",
        "=====================================================================================================\n",
        "This training script was originally developed and optimized for execution within Google Colab,\n",
        "relying heavily on Google Drive for persistent storage, Colab-specific authentication mechanisms,\n",
        "and other environment-dependent utilities. As a result, the initial implementation included\n",
        "Drive-mounted checkpoint directories, CSV logging to Drive, and secret-based Hugging Face login via\n",
        "Colab‚Äôs userdata API. While these components streamlined experimentation within a Colab workflow,\n",
        "they also made the script less portable and harder to reproduce in general compute environments\n",
        "such as local machines, cloud VMs, or managed training clusters.\n",
        "\n",
        "You can refactor the current version and remove the above mentioned Colab-specific assumptions,\n",
        "replacing them with environment-agnostic paths, standard Hugging Face authentication, and fully\n",
        "general dataset/model loading logic so the script can run consistently anywhere while retaining\n",
        "the same behavior and training methodology.\n",
        "=====================================================================================================\n",
        "'''\n",
        "# ==========================================\n",
        "# 1. Install Dependencies\n",
        "# ==========================================\n",
        "print(\"‚è≥ Installing libraries...\")\n",
        "!pip install -q -U transformers datasets trl accelerate huggingface_hub bitsandbytes\n",
        "'''\n",
        "The training was conducted using the following library versions at the time:\n",
        "Accelerate: 0.28.0\n",
        "Hugging Face Hub: 0.17.1\n",
        "Transformers: 4.57.3\n",
        "TRL: 0.25.1\n",
        "Pytorch: 2.9.0+cu126\n",
        "Datasets: 4.4.1\n",
        "Tokenizers: 0.22.1\n",
        "Bitsandbytes: 0.48.2 (it was used as everything here was done in BF16, SFT and RM were loaded in BF16)\n",
        "\n",
        "NOTE : Setting loss_type = 'ipo' in TRL's DPOTrainer's DPOConfig, enables IPO implementation\n",
        "'''\n",
        "import torch\n",
        "import os\n",
        "import csv\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainerCallback\n",
        ")\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "# ==========================================\n",
        "# 2. Setup Drive & Login\n",
        "# ==========================================\n",
        "print(\"\\nüìÇ Mounting Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/Qwen3-IPO-Training\"\n",
        "CHECKPOINT_DIR = f\"{DRIVE_ROOT}/checkpoints\"\n",
        "LOG_FILE_PATH = f\"{DRIVE_ROOT}/ipo_logs.csv\"\n",
        "\n",
        "# Create directories and initialize the CSV log header if missing\n",
        "if not os.path.exists(CHECKPOINT_DIR):\n",
        "    os.makedirs(CHECKPOINT_DIR)\n",
        "\n",
        "if not os.path.exists(LOG_FILE_PATH):\n",
        "    with open(LOG_FILE_PATH, mode='w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Step\", \"Epoch\", \"Loss\", \"Reward_Chosen\", \"Reward_Rejected\", \"Accuracy\", \"Margin\"])\n",
        "\n",
        "# Hugging Face Login\n",
        "print(\"\\nüîë Logging in...\")\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    login(token=hf_token, add_to_git_credential=True)\n",
        "    print(\"‚úÖ Logged in via Colab Secret.\")\n",
        "except:\n",
        "    login(add_to_git_credential=True)\n",
        "\n",
        "# ==========================================\n",
        "# 3. Configuration\n",
        "# ==========================================\n",
        "SFT_MODEL_ID = \"AIPlans/qwen3-0.6b-SFT-hs2\" # this works as policy(to be trained)\n",
        "OUTPUT_REPO  = \"your-username/qwen3-0.6b-IPO\" # this naming is arbitrary\n",
        "DATASET_NAME = \"Jennny/helpsteer2-helpfulness-preference\"  # this is a variant of the HelpSteer2 dataset having only the helpfulness attribute\n",
        "\n",
        "# All hyperparameters can be modified as suitable (A100 80GB was used at the time)\n",
        "BETA = 0.01               # for KL divergence\n",
        "LEARNING_RATE = 5e-7      # a lower learning rate as there are 3 epochs\n",
        "BATCH_SIZE = 8            # <--- Reduced to 8 for not hitting OOM\n",
        "GRAD_ACCUMULATION = 2     # <--- Effective Batch = 16\n",
        "EPOCHS = 3\n",
        "MAX_LENGTH = 2048\n",
        "\n",
        "# ==========================================\n",
        "# 4. Logging Callback\n",
        "# ==========================================\n",
        "class DriveLoggingCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs:\n",
        "            with open(LOG_FILE_PATH, mode='a', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([\n",
        "                    state.global_step,\n",
        "                    logs.get(\"epoch\", 0),\n",
        "                    logs.get(\"loss\", 0),\n",
        "                    logs.get(\"rewards/chosen\", 0),\n",
        "                    logs.get(\"rewards/rejected\", 0),\n",
        "                    logs.get(\"rewards/accuracies\", 0),\n",
        "                    logs.get(\"rewards/margins\", 0)\n",
        "                ])\n",
        "\n",
        "# ==========================================\n",
        "# 5. Dataset Loading & Formatting\n",
        "# ==========================================\n",
        "print(f\"\\nwv Loading dataset: {DATASET_NAME}...\")\n",
        "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
        "dataset = dataset.filter(lambda x: x[\"chosen_score\"] >= 3)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL_ID, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def format_ipo_triplets(example):\n",
        "    if isinstance(example['chosen'], list):\n",
        "        prompt = example['chosen'][0]['content']\n",
        "        chosen_response = example['chosen'][1]['content']\n",
        "        rejected_response = example['rejected'][1]['content']\n",
        "    else:\n",
        "        prompt = \"\"\n",
        "        chosen_response = str(example['chosen'])\n",
        "        rejected_response = str(example['rejected'])\n",
        "\n",
        "    return {\n",
        "        \"prompt\": f\"User: {prompt}\\n\\nAssistant:\",\n",
        "        \"chosen\": f\" {chosen_response}{tokenizer.eos_token}\",\n",
        "        \"rejected\": f\" {rejected_response}{tokenizer.eos_token}\"\n",
        "    }\n",
        "\n",
        "formatted_dataset = dataset.map(format_ipo_triplets, remove_columns=dataset.column_names)\n",
        "# Split: 95% Train, 5% Validation\n",
        "split_dataset = formatted_dataset.train_test_split(test_size=0.05, seed=42)\n",
        "print(f\"‚úÖ Dataset Ready: {len(split_dataset['train'])} Train | {len(split_dataset['test'])} Val\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. Load Policy Model\n",
        "# ==========================================\n",
        "print(\"\\nüß† Loading Policy Model (BF16)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    SFT_MODEL_ID,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.config.use_cache = False\n",
        "print(\"‚úÖ Model Loaded.\")\n",
        "\n",
        "# ==========================================\n",
        "# 7. Training\n",
        "# ==========================================\n",
        "training_args = DPOConfig(\n",
        "    output_dir=CHECKPOINT_DIR,\n",
        "    loss_type=\"ipo\",\n",
        "    beta=BETA,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=20,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    bf16=True,\n",
        "    max_length=MAX_LENGTH,\n",
        "    max_prompt_length=1024,\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=OUTPUT_REPO,\n",
        "    report_to=\"none\",\n",
        "    gradient_checkpointing=True,\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=None,\n",
        "    args=training_args,\n",
        "    train_dataset=split_dataset[\"train\"],\n",
        "    eval_dataset=split_dataset[\"test\"],\n",
        "    processing_class=tokenizer,\n",
        "    callbacks=[DriveLoggingCallback()],\n",
        ")\n",
        "\n",
        "print(\"\\nüöÄ Starting IPO Training...\")\n",
        "trainer.train()\n",
        "\n",
        "# ==========================================\n",
        "# 8. Final Save\n",
        "# ==========================================\n",
        "print(\"\\n‚òÅÔ∏è Pushing IPO Model to Hub...\")\n",
        "trainer.push_to_hub()\n",
        "print(f\"‚úÖ DONE! Model uploaded to: https://huggingface.co/{OUTPUT_REPO}\")"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "jDIPvv3v6Qel"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}