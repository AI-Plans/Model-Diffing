{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"If any model error arises, restart the kernel","metadata":{}},{"cell_type":"code","source":"pip install -q bitsandbytes trl ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T17:20:09.056191Z","iopub.execute_input":"2025-11-19T17:20:09.056482Z","iopub.status.idle":"2025-11-19T17:20:12.628495Z","shell.execute_reply.started":"2025-11-19T17:20:09.056453Z","shell.execute_reply":"2025-11-19T17:20:12.627489Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\nfrom collections import Counter\n\n# Load the dataset\ndataset = load_dataset(\"nvidia/HelpSteer2\", split=\"train\")\n\n# ---- Step 1: Remove samples where helpfulness == 3 ----\ndataset = dataset.filter(lambda x: x[\"helpfulness\"] != 3)\n\n# ---- Step 2: Create binary labels with >3 ----\ndataset = dataset.map(lambda x: {\"label\": x[\"helpfulness\"] > 3})\n\n# ---- Step 3: Prepare completion field ----\ndataset = dataset.map(\n    lambda x: {\"completion\": x[\"response\"]},\n    remove_columns=[\"response\"]\n)\n\n# ---- Step 4: Compute length of each example (prompt + completion) ----\ndef add_len(example):\n    prompt_text = example.get(\"prompt\", \"\") or example.get(\"completion\", \"\")  # Handle different possible field names\n    completion_text = example[\"completion\"]\n    example[\"total_length\"] = len(prompt_text) + len(completion_text)\n    return example\n\ndataset = dataset.map(add_len)\n\n# ---- Step 5: Split dataset by label ----\ntrue_set = dataset.filter(lambda x: x[\"label\"] == True)\nfalse_set = dataset.filter(lambda x: x[\"label\"] == False)\n\nprint(f\"Before balancing: True={len(true_set)}, False={len(false_set)}\")\n\n# ---- Step 6: Balance counts ----\nmin_count = min(len(true_set), len(false_set))\n\n# ---- Step 7: Sort each set by length (largest first) ----\ntrue_sorted = true_set.sort(\"total_length\", reverse=True)\nfalse_sorted = false_set.sort(\"total_length\", reverse=True)\n\n# ---- Step 8: Trim each set by keeping only the top min_count largest examples ----\ntrue_balanced = true_sorted.select(range(min_count))  # Keep first min_count examples (largest)\nfalse_balanced = false_sorted.select(range(min_count))  # Keep first min_count examples (largest)\n\n# ---- Step 9: Combine ----\nfrom datasets import concatenate_datasets\nbalanced_dataset = concatenate_datasets([true_balanced, false_balanced])\n\nprint(\"Final balanced stats:\", Counter(balanced_dataset[\"label\"]))\nprint(f\"Final dataset size: {len(balanced_dataset)}\")\ndataset = balanced_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T17:20:12.629580Z","iopub.execute_input":"2025-11-19T17:20:12.629872Z","iopub.status.idle":"2025-11-19T17:20:17.586256Z","shell.execute_reply.started":"2025-11-19T17:20:12.629843Z","shell.execute_reply":"2025-11-19T17:20:17.585387Z"}},"outputs":[{"name":"stdout","text":"Before balancing: True=8434, False=6013\nFinal balanced stats: Counter({True: 6013, False: 6013})\nFinal dataset size: 12026\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model\nfrom trl import KTOConfig, KTOTrainer\n\nmodel_name = \"Qwen/Qwen3-0.6B\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=\"bfloat16\",   # or torch.bfloat16\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    target_modules=[\"q_proj\", \"v_proj\"],  # or [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T17:20:17.587230Z","iopub.execute_input":"2025-11-19T17:20:17.587777Z","iopub.status.idle":"2025-11-19T17:20:36.527754Z","shell.execute_reply.started":"2025-11-19T17:20:17.587748Z","shell.execute_reply":"2025-11-19T17:20:36.527047Z"}},"outputs":[{"name":"stderr","text":"2025-11-19 17:20:22.625118: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763572822.648330     199 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763572822.655193     199 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc1b41fad9e54d009a48bd7780ac5881"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90a896301423482f9748a800eed8a03d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80b0e887a419445588b643e5a784eb9c"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"\nfrom trl import KTOConfig, KTOTrainer\n\nkto_config = KTOConfig(\n    report_to=\"none\",\n    output_dir=\"kto-out\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=8,\n    # max_length=512,\n    # max_prompt_length=384,\n    # key flag:\n    precompute_ref_log_probs=True,\n    bf16=True,\n    gradient_checkpointing=True,\n)\n\ntrainer = KTOTrainer(\n    model=model,                 # LoRA / QLoRA model\n    # ref_model=None,             # KTO will create / handle ref internally for precompute\n    args=kto_config,\n    processing_class=tokenizer,\n    train_dataset=dataset,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T17:20:36.529458Z","iopub.execute_input":"2025-11-19T17:20:36.529703Z","iopub.status.idle":"2025-11-20T03:07:18.604245Z","shell.execute_reply.started":"2025-11-19T17:20:36.529684Z","shell.execute_reply":"2025-11-20T03:07:18.603570Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/trl/trainer/kto_trainer.py:364: UserWarning: This trainer will soon be moved to trl.experimental and is a candidate for removal. If you rely on it and want it to remain, please share your comments here: https://github.com/huggingface/trl/issues/4223. Silence this warning by setting environment variable TRL_EXPERIMENTAL_SILENCE=1.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting prompt from train dataset:   0%|          | 0/12026 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a893f012310428bab2f1c8331b890d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset:   0%|          | 0/12026 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"350a349b81624a1daf4a5faf20365d89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/12026 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec778cff6e0b4890be584305836615e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing tokenized train dataset:   0%|          | 0/12026 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27ea39a4d6384649a9852bf7f5210873"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting KL train dataset:   0%|          | 0/12026 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbe35074fb2a4e7c84a22baf27d4427c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing tokenized train KL dataset:   0%|          | 0/12026 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6473589281854c43b4021554c692d6ab"}},"metadata":{}},{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train dataset reference log probs:   0%|          | 0/6013 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eb57b21384e436b80cf16495ea899df"}},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2256' max='2256' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2256/2256 8:31:19, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.504800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.503700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.494300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.494100</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.488900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.498400</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.493400</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.496300</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.495700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.499000</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.503700</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.499200</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.503700</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.508700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.498200</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.515600</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.511000</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.520800</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.517100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.516300</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.520000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.518600</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.529400</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.513600</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.520400</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.522600</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.523500</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.504600</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.520200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.512200</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.537500</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.543900</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.528900</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.537300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.528500</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.548900</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.517500</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.499900</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.411300</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.506100</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.522200</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.517000</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.509800</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.498100</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.511700</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.525100</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.490500</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.498200</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.497400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.503100</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.516400</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.495900</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.517100</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.502800</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.502900</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.518700</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.500900</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.493300</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.501400</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.504600</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.495200</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.496600</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.498300</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.501500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.498600</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.493100</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.499500</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.501600</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.503100</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.505000</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.505500</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.502400</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.504400</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.502700</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.501800</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.478700</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.468800</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.462900</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.466500</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.481000</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.459700</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.483800</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.483000</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.482000</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.495000</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.493000</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.493800</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.510600</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.489600</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.502300</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.509100</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.519700</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.507300</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.513600</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.519100</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.509300</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.521100</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.512200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.512900</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.511900</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.512900</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.504900</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.506300</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.503900</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.527700</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.543100</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.524800</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.514600</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.527400</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.534100</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.500700</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.518000</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.422400</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.503800</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.512700</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.527400</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.506100</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.502400</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.516200</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.521000</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.502800</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.497200</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.504300</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.507200</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.510300</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.498400</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.521500</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.506900</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.504100</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.520800</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.508300</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.498200</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.499500</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.506000</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.498800</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.494200</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.502500</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.493300</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.509200</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.491600</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.495600</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.495600</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.505300</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.506000</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.508000</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.503700</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.504100</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.504400</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.503800</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.500300</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.478300</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.455700</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.434400</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.450300</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.467400</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.457600</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.472900</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.471100</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.478900</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.483400</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.489700</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.488400</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.505600</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.488800</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.501900</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.498000</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.519400</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.500200</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.512100</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.500200</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.510100</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.518500</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.508500</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.507600</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.507800</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.510300</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.494900</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.502500</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.501000</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.514200</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.526500</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.522800</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.506400</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.526000</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.515600</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.503600</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.514100</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.433400</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.488900</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.508200</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.539600</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.519800</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.503700</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.515800</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.527700</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.504100</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.494800</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.508400</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.506800</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>0.509800</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.509500</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>0.514300</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.500500</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.505600</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.520900</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>0.511500</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.496200</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>0.506000</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.497200</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>0.498600</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>0.496400</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>0.496100</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>0.492600</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.503400</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>0.490600</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>0.489600</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>0.494500</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>0.498100</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.506100</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>0.504600</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>0.503700</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>0.503700</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>0.502400</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.505600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2256, training_loss=0.5031756252473127, metrics={'train_runtime': 30704.2814, 'train_samples_per_second': 1.175, 'train_steps_per_second': 0.073, 'total_flos': 0.0, 'train_loss': 0.5031756252473127, 'epoch': 3.0})"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"The dataset is imbalanced. To compensate we change undesirable_weight parameter to 2. Read more here- https://huggingface.co/docs/trl/main/en/kto_trainer#imbalanced-data","metadata":{}},{"cell_type":"code","source":"trainer.save_model(\"Qwen3-0.6B-KTO\")\ntokenizer.save_pretrained(\"Qwen3-0.6B-KTO\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T03:07:18.633198Z","iopub.execute_input":"2025-11-20T03:07:18.633398Z","iopub.status.idle":"2025-11-20T03:07:19.524310Z","shell.execute_reply.started":"2025-11-20T03:07:18.633381Z","shell.execute_reply":"2025-11-20T03:07:19.523564Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"('Qwen3-0.6B-KTO/tokenizer_config.json',\n 'Qwen3-0.6B-KTO/special_tokens_map.json',\n 'Qwen3-0.6B-KTO/chat_template.jinja',\n 'Qwen3-0.6B-KTO/vocab.json',\n 'Qwen3-0.6B-KTO/merges.txt',\n 'Qwen3-0.6B-KTO/added_tokens.json',\n 'Qwen3-0.6B-KTO/tokenizer.json')"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"repo_id = \"AIPlans/Qwen3-0.6B-KTO\"\n\nmodel.push_to_hub(repo_id)\ntokenizer.push_to_hub(repo_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T03:07:19.849076Z","iopub.execute_input":"2025-11-20T03:07:19.849303Z","iopub.status.idle":"2025-11-20T03:07:32.922289Z","shell.execute_reply.started":"2025-11-20T03:07:19.849285Z","shell.execute_reply":"2025-11-20T03:07:32.921651Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c31aa1377b04ea3b0ed88856c023428"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2b788cb1fce4bfb8f1471e1258d2731"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"166879d623c04db5bd20ed5d084915f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"232921022f4d4e53a7c0b7bb04fb84c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5710501c495a436fb10a3ca94d21a595"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/AIPlans/Qwen3-0.6B-KTO/commit/02bceae8ae07f474841d264a69de6175148b1721', commit_message='Upload tokenizer', commit_description='', oid='02bceae8ae07f474841d264a69de6175148b1721', pr_url=None, repo_url=RepoUrl('https://huggingface.co/AIPlans/Qwen3-0.6B-KTO', endpoint='https://huggingface.co', repo_type='model', repo_id='AIPlans/Qwen3-0.6B-KTO'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}