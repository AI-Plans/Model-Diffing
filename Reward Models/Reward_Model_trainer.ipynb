{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Reward Model Training\n",
        "'''\n",
        "===========================================================================================================\n",
        "This training script was originally developed and optimized for execution within Google Colab,\n",
        "relying on colab-specific authentication mechanisms, and other environment-dependent utilities.\n",
        "As a result, the initial implementation included secret-based Hugging Face login via Colab‚Äôs userdata API.\n",
        "While these components streamlined experimentation within a Colab workflow, this also made the script\n",
        "less portable and harder to reproduce in general compute environments such as local machines, cloud VMs,\n",
        "or managed training clusters.\n",
        "\n",
        "You can refactor the current version and remove the above mentioned Colab-specific assumptions,\n",
        "replacing them with environment-agnostic paths, standard Hugging Face authentication, and fully\n",
        "general dataset/model loading logic so the script can run consistently anywhere while retaining\n",
        "the same behavior and training methodology.\n",
        "===========================================================================================================\n",
        "'''\n",
        "# ==========================================\n",
        "# 1. Install Dependencies\n",
        "# ==========================================\n",
        "print(\"‚è≥ Installing libraries...\")\n",
        "!pip install -q -U transformers datasets trl accelerate huggingface_hub\n",
        "'''\n",
        "The training was conducted using the following library versions at the time:\n",
        "Accelerate: 0.28.0\n",
        "Hugging Face Hub: 0.17.1\n",
        "TRL: 0.25.1\n",
        "Transformers: 4.57.3\n",
        "Pytorch: 2.9.0+cu126\n",
        "Datasets: 4.4.1\n",
        "Tokenizers: 0.22.1\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "import datasets\n",
        "import trl\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "from trl import RewardTrainer, RewardConfig\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# ==========================================\n",
        "# 2. Setup & Login\n",
        "# ==========================================\n",
        "print(\"\\nüîë Logging in...\")\n",
        "try:\n",
        "    # Ensure you have a secret named 'HF_TOKEN' in Colab (Key icon on left)\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    login(token=hf_token, add_to_git_credential=True)\n",
        "    print(\"‚úÖ Logged in via Colab Secret.\")\n",
        "except Exception:\n",
        "    print(\"‚ö†Ô∏è Secret 'HF_TOKEN' not found. Falling back to manual input.\")\n",
        "    login(add_to_git_credential=True)\n",
        "\n",
        "# ==========================================\n",
        "# 3. Configuration\n",
        "# ==========================================\n",
        "# Model & Data\n",
        "MODEL_NAME = \"Qwen/Qwen3-0.6B-Base\"\n",
        "DATASET_NAME = \"Jennny/helpsteer2-helpfulness-preference\" # this is a variant of the HelpSteer2 dataset having only the helpfulness attribute\n",
        "REPO_NAME = \"your-username/qwen3-0.6b-RM\" # model name is arbitrary\n",
        "\n",
        "# All hyperparameters can be modified as suitable\n",
        "MAX_LENGTH = 2048\n",
        "BATCH_SIZE = 16\n",
        "GRAD_ACCUMULATION = 2 # Effective Batch Size = 32\n",
        "LEARNING_RATE = 5e-6\n",
        "\n",
        "# ==========================================\n",
        "# 4. Dataset Loading & Filtering\n",
        "# ==========================================\n",
        "print(f\"\\nwv Loading dataset: {DATASET_NAME}...\")\n",
        "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
        "\n",
        "# Identify the correct score column (varies by dataset version)\n",
        "score_col = \"chosen_score\"\n",
        "if score_col not in dataset.column_names:\n",
        "    possible = [c for c in dataset.column_names if \"score\" in c or \"rating\" in c]\n",
        "    if possible: score_col = possible[0]\n",
        "\n",
        "# We filter out low-quality data so the model only learns from \"helpful\" examples (Score >= 3)\n",
        "try:\n",
        "    original_len = len(dataset)\n",
        "    dataset = dataset.filter(lambda x: x[score_col] >= 3)\n",
        "    print(f\"‚úÖ Filtered dataset (Score >= 3): {original_len} -> {len(dataset)} samples\")\n",
        "except KeyError:\n",
        "    print(\"‚ö†Ô∏è Warning: Score column not found. Skipping filter.\")\n",
        "\n",
        "# ==========================================\n",
        "# 5. Robust Preprocessing\n",
        "# ==========================================\n",
        "print(\"\\n‚öôÔ∏è Preprocessing...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    new_examples = {\n",
        "        \"input_ids_chosen\": [],\n",
        "        \"attention_mask_chosen\": [],\n",
        "        \"input_ids_rejected\": [],\n",
        "        \"attention_mask_rejected\": [],\n",
        "        \"chosen\": [],   # TRL needs these as strings for sanity checks\n",
        "        \"rejected\": []  # TRL needs these as strings for sanity checks\n",
        "    }\n",
        "\n",
        "    for chosen_raw, rejected_raw in zip(examples[\"chosen\"], examples[\"rejected\"]):\n",
        "        # Handle cases where data is a list of dicts (Conversation) vs string\n",
        "        if isinstance(chosen_raw, list):\n",
        "            prompt_text = chosen_raw[0]['content']\n",
        "            chosen_response = chosen_raw[1]['content']\n",
        "            rejected_response = rejected_raw[1]['content']\n",
        "        else:\n",
        "            # Fallback if already string\n",
        "            prompt_text = \"\"\n",
        "            chosen_response = str(chosen_raw)\n",
        "            rejected_response = str(rejected_raw)\n",
        "\n",
        "        # Apply manual chat template (User/Assistant) for the Base model\n",
        "        # Note: We do NOT add EOS manually here. RewardTrainer adds it automatically.\n",
        "        chosen_text = f\"User: {prompt_text}\\n\\nAssistant: {chosen_response}\"\n",
        "        rejected_text = f\"User: {prompt_text}\\n\\nAssistant: {rejected_response}\"\n",
        "\n",
        "        # Tokenize\n",
        "        tokenized_chosen = tokenizer(chosen_text, truncation=True, max_length=MAX_LENGTH)\n",
        "        tokenized_rejected = tokenizer(rejected_text, truncation=True, max_length=MAX_LENGTH)\n",
        "\n",
        "        new_examples[\"input_ids_chosen\"].append(tokenized_chosen[\"input_ids\"])\n",
        "        new_examples[\"attention_mask_chosen\"].append(tokenized_chosen[\"attention_mask\"])\n",
        "        new_examples[\"input_ids_rejected\"].append(tokenized_rejected[\"input_ids\"])\n",
        "        new_examples[\"attention_mask_rejected\"].append(tokenized_rejected[\"attention_mask\"])\n",
        "\n",
        "        # Store raw text strings for TRL (Fixes KeyError: 'chosen')\n",
        "        new_examples[\"chosen\"].append(chosen_text)\n",
        "        new_examples[\"rejected\"].append(rejected_text)\n",
        "\n",
        "    return new_examples\n",
        "\n",
        "# Apply processing and remove old columns to avoid schema conflicts, then split\n",
        "processed_dataset = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "split_dataset = processed_dataset.train_test_split(test_size=0.1) # 10% prompts were used for in between loss validation check\n",
        "\n",
        "# ==========================================\n",
        "# 6. Model Loading & Initialization\n",
        "# ==========================================\n",
        "print(f\"\\nüß† Loading Model: {MODEL_NAME} (FP32)...\")\n",
        "# num_labels=1 converts the model head to output a single scalar score\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=1,\n",
        "    torch_dtype=torch.float32, # FP32 for stability\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Zero-initialize the score head for stability\n",
        "# This ensures the model starts with a neutral reward bias (0.0), preventing initial instability\n",
        "print(\"‚öñÔ∏è Stabilizing: Zero-initializing score weights...\")\n",
        "for name, param in model.named_parameters():\n",
        "    if \"score\" in name or \"classifier\" in name:\n",
        "        nn.init.constant_(param, 0.0)\n",
        "\n",
        "# ==========================================\n",
        "# 7. Training\n",
        "# ==========================================\n",
        "training_args = RewardConfig(\n",
        "    output_dir=REPO_NAME,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=20,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    max_length=MAX_LENGTH,\n",
        "    bf16=False, # FP32\n",
        "    fp16=False, # FP32\n",
        "    remove_unused_columns=False, # CRITICAL: Prevents Trainer from dropping 'chosen'/'rejected' text columns\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=REPO_NAME,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = RewardTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer, # 'processing_class' replaces 'tokenizer' in newer TRL versions (v0.12+)\n",
        "    args=training_args,\n",
        "    train_dataset=split_dataset[\"train\"],\n",
        "    eval_dataset=split_dataset[\"test\"],\n",
        ")\n",
        "\n",
        "print(\"\\nüöÄ Starting Training...\")\n",
        "trainer.train()\n",
        "\n",
        "# ==========================================\n",
        "# 8. Push to Hub\n",
        "# ==========================================\n",
        "print(\"\\n‚òÅÔ∏è Pushing final model to Hub...\")\n",
        "trainer.push_to_hub()\n",
        "print(f\"‚úÖ DONE! Model uploaded to: https://huggingface.co/{REPO_NAME}\")"
      ],
      "metadata": {
        "id": "OUxduVkLQuw7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
